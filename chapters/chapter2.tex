\chapter{Literature Review}
\label{chap:lr}
\chaptermark{Second Chapter Heading}

This chapter is devoted to a review of the literature, which is fundamental for solving the tasks set in our study. The focus is on the use of unsupervised contrastive learning methods to extract features from various datasets, for example, in medicine. 

Section I describes the principles of contrastive learning and its application in areas with limited labeled data. Section II covers the main methods and discusses advances in negative sampling.

\section{Unsupervised Contrastive Learning Approaches}
Contrastive learning is a deep learning technique that is able to derive meaningful representations from data without relying on labels. It focuses on contrasting positive and negative sample pairs and is particularly useful in the field of medical imaging, where labeled data can be both scarce and costly.

\subsection{Foundations and Methodological Innovations}

In their work on SimCLR, Chen et al. (2020) \cite{tsimcne} described an algorithm for effective contrastive learning and further visualization of data in 2D. SimCLR generates augmented versions of one image, treating them as "positive" pairs and other images as "negative" pairs. Then, the model learns to combine the features of positive pairs and separate the features of negative pairs in the embedding space: a basic neural network extracts elements from images, and then a projection head maps these elements to the space where contrast loss is applied. The focus on different augmentation strategies helps the model to remember the important characteristics needed to understand complex medical images. In addition, using many images increases the number of negative examples, increasing the model's ability to distinguish between different images.

The Momentum Contrast (MoCo) framework by He et al. (2020) \cite{moco} further advances the field by introducing a dynamic queue of representations and a momentum-updated encoder. They offer the solution to problems related to time variability and diversity of patient medical data sets. Similarly, a study by BYL by Grill et al. (2020) \cite{byol} gets rid of the need for explicit negative indicators by implementing a new approach that works well with different sets of medical data.

The work by Azizi et al. (2021) \cite{azizi} focuses on using large-scale self-supervised models for medical image classification. It shows how big self-learning models can help better use the vast number of medical images that do not have labels and improve and automate medical diagnosis tools.

Sharma et al. (2024) \cite{sharma2023auc} addressed the limitations associated with large batch sizes in contrastive learning. They introduced the AUC-Contrastive Learning (AUC-CL) framework, which integrates the area under the ROC curve (AUC) maximization with contrastive learning principles. This approach reduces the dependency on large batch sizes, a common challenge in self-supervised learning frameworks such as SimCLR and MoCo.

\subsection{Advances in Negative Sampling}
The quality of negative samples is crucial in contrastive learning because it directly influences the learned representations. Recent advancements aim to address biases in negative sampling to enhance model performance.

A significant advancement is the Positive-Unlabeled Noise Contrastive Estimation (puNCE) method by Acharya et al. \cite{pucl}. This method assigns weights to each sample, with labeled positives given full weight and unlabeled samples considered both positive and negative with probabilities based on the class. This algorithm improves representation quality, particularly with limited labeled data.

Debiased Contrastive Learning (DCL) by Chuang et al. \cite{chuang2020debiased} reduces sampling bias by adjusting the importance of negative samples based on difficulty, making a training process more balanced.

Adding to these advancements, Robinson et al. \cite{robinson2020contrastive} introduce a method that improves the sampling of hard negatives by creating a tunable distribution that prefers negative pairs whose representations are currently very similar. This approach addresses the challenge of using hard negatives without supervision, enhancing the performance of contrastive learning models without additional computational overhead.

% \subsection{("""DRAFT""") Implications for Medical Imaging and more}
% While the methodologies discussed have broad applications, their implications for medical data analysis are particularly noteworthy. The enhancements in negative sampling techniques, combined with the foundational principles of contrastive learning, contribute to developing sophisticated models capable of addressing the unique challenges of medical datasets. These advancements pave the way for extracting more clinically relevant features from medical images, facilitating the development of advanced diagnostic and prognostic tools.

\subsection{Adaptability and Enhancement of Unsupervised Learning of Contrastive Learning Across Diverse Datasets}

The success of contrastive learning in medical imaging depends on the strategic use of data augmentations, as underscored by Tian et al. (2020) \cite{tian2022understanding}. These augmentations generate diverse perspectives of the same image, which is crucial for highlighting key features. Van der Sluijs et al. delve into their impact on learning representations \cite{van2024exploring}. Specifically, Kang et al. suggest rotations, flips, and color adjustments relevant to tissue staining using medical images' spatial and visual characteristics \cite{kang2023benchmarking}. Moreover, Using adjacent patches as matching examples, as discussed by Li et al. (2021) \cite{li2021dual} and Wang et al. (2021) \cite{wang2021transpath}, takes advantage of the natural layout of cells in histopathological samples.


A study by Van Gansbeke et al. (2021) \cite{van2021revisiting} shows the flexibility and effectiveness of contrastive unsupervised learning when using the MoCo framework across different types of datasets. They underscore the robust performance of this learning approach in various settings. Also, using data augmentations like random cropping helps ensure that similar pairs retain common information, which supports effective feature learning.

Van Gansbeke et al. also discuss how small adjustments, such as implementing multi-scale cropping, introducing stronger augmentations, and adding nearest neighbors, can enhance the representations learned. These modifications make the representations more adaptable to a wide array of tasks. The paper also notes that the MoCo framework, with a multi-crop approach, produces representations that can be directly used for tasks like semantic segment retrieval without further fine-tuning.

Zang et al. introduce a new technique known as DiffAug \cite{zang2023boosting}, which enhances the potential of unsupervised learning models. By generating diverse training examples, DiffAug enriches the learning environment, allowing models to capture a broader array of features present in complex datasets.

\section{Dimensionality reduction for visualisation}

This section explores several pivotal algorithms that have significantly contributed to the field, particularly in the context of 2D visualization of datasets.

One of the foundational techniques in dimensionality reduction is the Locally Linear Embedding (LLE), introduced by Roweis and Saul (2000) \cite{roweis2000nonlinear}. LLE is a method of studying the structure of the data that focuses on keeping local groupings unchanged, which is really useful when working with confusing or distorted data. However, LINE may not always be useful for understanding the general layout of data, which may make it less useful for tasks that require a complete overview of the dataset."

Building on the concept of neighborhood preservation, Stochastic Neighbor Embedding (SNE), developed by Hinton and Roweis (2003) \cite{hinton2002stochastic}, introduced a probabilistic framework for dimensionality reduction. SNE aims to maintain similarities between pairs in a low-dimensional space, producing the visualization of clusters or groups within the data. Despite its strengths, SNE's cost function is difficult to optimize, and the algorithm is prone to a "crowding problem," where dissimilar data points collapse onto each other in the reduced space.

To address these limitations, t-SNE (t-distributed Stochastic Neighbor Embedding), as proposed by van der Maaten and Hinton (2008) \cite {van2008visualizing}, uses a t-distribution to measure similarities in the low-dimensional space. This adjustment solves the crowding problem and has made t-SNE a popular choice for visualizing complex datasets. Nevertheless, t-SNE's computational complexity and tendency to form arbitrary clusters due to its sensitivity to hyperparameters can be problematic, especially when interpreting the results scientifically.

In response to the scalability and interpretability challenges of t-SNE, UMAP (Uniform Manifold Approximation and Projection), introduced by McInnes et al. (2018) \cite{mcinnes2018umap}, offers an alternative. UMAP reduces computational time and better preserves the global structure of the data, making a more interpretable mapping from high-dimensional to low-dimensional space. However, UMAP's reliance on k-nearest neighbor graphs can sometimes lead to less defined clusters, especially in datasets with intricate topologies.

LargeVis (Tang et al., 2016) \cite{tang2016visualizing} and TriMap (Amid and Warmuth, 2019) \cite {amid2019trimap} further advance in the field, introducing new approaches to balance the preservation of local and global data structures. LargeVis, for example, succeded in handling very large datasets through its efficient neighborhood selection algorithm. TriMap, on the other hand, uses a triplet-based loss function to maintain global relationships.

Parametric mappings is an important development that makes these visualization techniques more adaptable to new data points. Van der Maaten (2009) \cite{van2009learning} explored this idea in the context of t-SNE and produced a solution that allows embedding out-of-sample points without re-running the entire algorithm. This parametric approach is promising for real-time data analysis and interactive visualization applications, although it can sometimes sacrifice the detailed structure obtained by nonparametric methods.